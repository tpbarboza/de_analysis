{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:s3nh4n3t@database-1.chcqqyli2cjd.us-east-2.rds.amazonaws.com:5432/olist')\n",
    "conn = engine.connect()\n",
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets truncate all tables so this can be reexecuted without duplicating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f82e5224e48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\"\"\"truncate table order_review CASCADE;\n",
    "truncate table order_payment CASCADE; \n",
    "truncate table payment_type CASCADE;\n",
    "truncate table order_item CASCADE;\n",
    "truncate table \"order\" CASCADE;\n",
    "truncate table order_status CASCADE;\n",
    "truncate table product CASCADE;\n",
    "truncate table internationalization CASCADE;\n",
    "truncate table \"language\" CASCADE;\n",
    "truncate table category CASCADE;\n",
    "truncate table customer CASCADE;\n",
    "truncate table seller CASCADE;\n",
    "truncate table geolocation CASCADE;\n",
    "truncate table zip CASCADE;\n",
    "truncate table city CASCADE;\n",
    "truncate table state CASCADE;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seller = pd.read_csv('../datasets/olist_sellers_dataset.csv')\n",
    "df_product = pd.read_csv('../datasets/olist_products_dataset.csv')\n",
    "df_category_translation = pd.read_csv('../datasets/product_category_name_translation.csv')\n",
    "df_customers = pd.read_csv('../datasets/olist_customers_dataset.csv')\n",
    "df_geolocation = pd.read_csv('../datasets/olist_geolocation_dataset.csv')\n",
    "df_order = pd.read_csv('../datasets/olist_orders_dataset.csv')\n",
    "df_order_item = pd.read_csv('../datasets/olist_order_items_dataset.csv')\n",
    "df_order_payment = pd.read_csv('../datasets/olist_order_payments_dataset.csv')\n",
    "# The next dataframe has strings without delimiters which is breaking import.\n",
    "# getting all cols as string to perform basic checks\n",
    "df_order_review = pd.read_csv('../datasets/olist_order_reviews_dataset.csv', dtype=str, skipinitialspace = True, quotechar = '\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize zip, city, state and get all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets normalize data on dataframes\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode().translate(table).upper().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets normalize data on dataframes\n",
    "df_geolocation['geolocation_city'] = df_geolocation['geolocation_city'].map(lambda x: normalize_text(x))\n",
    "df_geolocation['geolocation_state'] = df_geolocation['geolocation_state'].map(lambda x: normalize_text(x))\n",
    "df_seller['seller_city'] = df_seller['seller_city'].map(lambda x: normalize_text(x))\n",
    "df_seller['seller_state'] = df_seller['seller_state'].map(lambda x: normalize_text(x))\n",
    "df_customers['customer_city'] = df_customers['customer_city'].map(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode().translate(table).upper().strip())\n",
    "df_customers['customer_state'] = df_customers['customer_state'].map(lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode().translate(table).upper().strip())\n",
    "\n",
    "# Now lets get all unique zip/city/state\n",
    "\n",
    "df_all_zip = df_customers.iloc[:, 2:5]\n",
    "df_all_zip.columns = ['zip_prefix','city', 'state']\n",
    "df_tmp_zip = df_seller.iloc[:, 1:4]\n",
    "df_tmp_zip.columns = ['zip_prefix','city', 'state']\n",
    "df_all_zip = pd.concat([df_all_zip, df_tmp_zip])\n",
    "df_tmp_zip = df_geolocation.drop(['geolocation_lat', 'geolocation_lng'], axis=1)\n",
    "df_tmp_zip.columns = ['zip_prefix','city', 'state']\n",
    "df_all_zip = pd.concat([df_all_zip, df_tmp_zip] )\n",
    "df_all_zip = df_all_zip.groupby(['zip_prefix','city', 'state']).count().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORY\n",
    "\n",
    "**get all categories from products dataset to ensure every possibel category in inserted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = df_product.product_category_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f82a82631d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = 'BEGIN;\\n'\n",
    "for cat in category_list:\n",
    "    sql += f\"insert into category(name) values('{cat}');\\n\"\n",
    "sql += 'COMMIT;'\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LANGUAGE\n",
    "**for now I'll insert only english, which is the only translation present in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f82b4a52a58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"insert into language(code) values('EN-US');\"\n",
    "conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"select * from language where code = 'EN-US'\"\n",
    "result = conn.execute(sql)\n",
    "lang_id, _ = result.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERNATIONALIZATION\n",
    "\n",
    "**Lets insert the translations for product categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'BEGIN;\\n'\n",
    "for index, row in df_category_translation.iterrows():\n",
    "    sql += f\"\"\"insert into internationalization(reference_table, reference_id, language_id, text) \n",
    "            VALUES('category', (select id from category where name='{row['product_category_name']}'), {lang_id}, '{row['product_category_name_english']}');\\n\"\"\"            \n",
    "sql += 'COMMIT;'\n",
    "result = conn.execute(sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = df_all_zip.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'BEGIN;\\n'\n",
    "for state in state_list:\n",
    "    sql += f\"\"\"insert into state(name) VALUES('{state}');\\n\"\"\"            \n",
    "sql += 'COMMIT;'\n",
    "result = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = df_all_zip.drop(['zip_prefix'], axis=1).groupby(['city', 'state']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'BEGIN;\\n'\n",
    "for index, row in df_cities.iterrows():\n",
    "    sql += f\"\"\"insert into city(name, state_id) VALUES('{row['city']}', (select id from state where name='{row['state']}'));\\n\"\"\"\n",
    "sql += 'COMMIT;'\n",
    "result = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'BEGIN;\\n'\n",
    "for index, row in df_all_zip.iterrows():\n",
    "    sql += f\"insert into zip(prefix, city_id) VALUES('{row['zip_prefix']}', (select id from city where name='{row['city']}' and state_id=(select id from state where name ='{row['state']}')));\\n\"\n",
    "sql += 'COMMIT;'\n",
    "result = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"select z.id zip_id, cast(z.prefix as int8) geolocation_zip_code_prefix, \n",
    "c.name geolocation_city, s.\"name\" geolocation_state\n",
    "from zip as z\n",
    "join city as c on z.city_id = c.id\n",
    "join state as s on c.state_id = s.id\n",
    "\"\"\"\n",
    "df_zip_id = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_geo_zip = pd.merge(df_geolocation, df_zip_id, how='left', on=['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo_zip1 = df_geo_zip.drop(['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'], axis=1)\n",
    "df_geo_zip1.columns = ['latitude','longitude','zip_id']\n",
    "df_geo_zip1.to_sql('geolocation', conn, if_exists='append', chunksize = 1000, index=False, method='multi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seller_1 = pd.merge(df_seller, df_zip_id, how='left', left_on=['seller_zip_code_prefix', 'seller_city', 'seller_state'], right_on=['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'])\n",
    "df_seller_1 = df_seller_1.drop(['seller_city', 'seller_state', 'seller_zip_code_prefix', 'geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'], axis=1)\n",
    "df_seller_1.columns = ['id', 'zip_id']\n",
    "df_seller_1.to_sql('seller', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUSTOMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_1 = pd.merge(df_customers, df_zip_id, how='left', left_on=['customer_zip_code_prefix', 'customer_city', 'customer_state'], right_on=['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'])\n",
    "df_customer_1 = df_customer_1.drop(['customer_city', 'customer_state', 'customer_zip_code_prefix', 'geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'], axis=1)\n",
    "df_customer_1.columns = ['id', 'unique_id', 'zip_id']\n",
    "df_customer_1.to_sql('customer', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_id = pd.read_sql(\"\"\"select id as category_id, name as category_name from category\"\"\", conn)\n",
    "df_product_1 = pd.merge(df_product, df_cat_id, how='left', left_on=['product_category_name'], right_on=['category_name'])\n",
    "df_product_1 = df_product_1.drop(['product_category_name','category_name'], axis=1)\n",
    "df_product_1.columns = ['id', 'name_length', 'description_length', 'photos_qty', 'weight_g', 'length_cm', 'height_cm', 'width_cm', 'category_id']\n",
    "df_product_1.to_sql('product', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f82a1878588>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_list = df_order.order_status.unique()\n",
    "sql = 'BEGIN;\\n'\n",
    "for cat in status_list:\n",
    "    sql += f\"insert into order_status(description) values('{cat}');\\n\"\n",
    "sql += 'COMMIT;'\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_1 = df_order.copy()\n",
    "df_order_1.columns = [column_name.replace('order_', '') for column_name in df_order.columns]\n",
    "df_status_id = pd.read_sql(\"\"\"select id as status_id, description as status from order_status\"\"\", conn)\n",
    "df_order_1 = pd.merge(df_order_1, df_status_id, how='left', on=['status'])\n",
    "df_order_1 = df_order_1.drop(['status'], axis=1)\n",
    "df_order_1.to_sql('order', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER ITEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_item_1 = df_order_item.copy()\n",
    "df_order_item_1.columns = ['order_id', 'item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value']\n",
    "df_order_item_1.to_sql('order_item', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PAYMENT TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f82a0a57ef0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payment_type_list = df_order_payment.payment_type.unique()\n",
    "sql = 'BEGIN;\\n'\n",
    "for cat in payment_type_list:\n",
    "    sql += f\"insert into payment_type(description) values('{cat}');\\n\"\n",
    "sql += 'COMMIT;'\n",
    "conn.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER PAYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_payment_1 = df_order_payment.copy()\n",
    "df_order_payment_1.columns = ['order_id', 'sequential', 'payment_type', 'installments', 'value']\n",
    "df_payment_type_id = pd.read_sql(\"\"\"select id as payment_type_id, description as payment_type from payment_type\"\"\", conn)\n",
    "df_order_payment_1 = pd.merge(df_order_payment_1, df_payment_type_id, how='left', on=['payment_type'])\n",
    "df_order_payment_1 = df_order_payment_1.drop(['payment_type'], axis=1)\n",
    "df_order_payment_1.to_sql('order_payment', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDER REVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting a total of 60 reviews which have bad formmating due to time constraints\n"
     ]
    }
   ],
   "source": [
    "df_order_review_1 = df_order_review.copy()\n",
    "print(f\"Deleting a total of {df_order_review[df_order_review['Unnamed: 7'].notnull()]['review_id'].count()} reviews which have bad formmating due to time constraints\")\n",
    "df_order_review_1 = df_order_review_1.drop(df_order_review_1[df_order_review_1['Unnamed: 7'].notnull()].index)\n",
    "df_order_review_1.columns = ['review_id'] + [column_name.replace('review_', '').replace('creation_', 'create_') for column_name in df_order_review_1.columns if column_name != 'review_id']\n",
    "df_order_review_1.columns\n",
    "df_order_review_1 = df_order_review_1.iloc[:, 0:7]\n",
    "df_order_review_1.to_sql('order_review', conn, if_exists='append', chunksize = 1000, index=False, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
